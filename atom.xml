<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Alex Sinfarosa</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://.alexsinfarosa.com/"/>
  <updated>2016-11-18T19:49:54.000Z</updated>
  <id>http://.alexsinfarosa.com/</id>
  
  <author>
    <name>Alex Sinfarosa</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Logistic Regression Modeling of Real Estate Properties</title>
    <link href="http://.alexsinfarosa.com/2016/06/16/Logistic-Regression-Modeling-of-Real-Estate-Properties/"/>
    <id>http://.alexsinfarosa.com/2016/06/16/Logistic-Regression-Modeling-of-Real-Estate-Properties/</id>
    <published>2016-06-16T21:08:35.000Z</published>
    <updated>2016-11-18T19:49:54.000Z</updated>
    
    <content type="html"><![CDATA[<p>I was recently tasked with discovering some additional information from a set of housing data in the upstate New York region that could help elucidate why some properties sell over others. Given two <code>csv</code> files with data from active and sold properties, I immediately thought how cool it would be to build a predictive model using logistic regression! I built the whole thing in R, too, which gave me more experience with the software, and surprised me in its simplicity and power. I was limited by my dataset, as it was not as large as I would have hoped for, but nevertheless, I got right down to work.</p>
<a id="more"></a>
<h2 id="Data-Data-Data…"><a href="#Data-Data-Data…" class="headerlink" title="Data Data Data…"></a>Data Data Data…</h2><p>Given the data for a particular region (where there is no need to filter the properties as the location variable is the same) I had a <code>csv</code> of the properties that have sold and another one detailing the properties that remain active. The relevant characteristics of the properties were – market price, address, days on market, square footage, acres of lot, year built, bedrooms, baths, and yearly property taxes.</p>
<h2 id="Dimensionality-Reduction"><a href="#Dimensionality-Reduction" class="headerlink" title="Dimensionality Reduction"></a>Dimensionality Reduction</h2><p>To build my model, it was vital to understand which variables (the characteristics of the properties listed above) contributed towards the likelihood of the property being sold (for example, is a property with higher taxes than another home close by, controlling for similarity in price and size less likely to sell?). Therefore, I did a prior examination of these relationships (essentially a dimensionality reduction, where the variables that were uncorrelated are determined as not being essential and thus omitted from the eventual model). R allows you to quickly do a scatter plot matrix of all pair-wise combinations of the variables. Having a horizontal or vertical slope was clear indication of this.</p>
<p><img src="graph1.png" alt="first graph"></p>
<p><em>Relation of variables of active properties: Horizontal and vertical slopes can be ruled out as having no effect on outcome, in preparation for our model.</em></p>
<p>Active Properties - The variables (in this case, market price, above ground square footage, acres, year built, bedrooms, bathrooms, and, total property taxes) can be found in a diagonal line from top left to bottom right. Each variable is then plotted against each other. For example, the first square in the second column next to the list price square, is an individual scatterplot of market price and above ground square footage, with price as the X-axis and above square footage as the Y-axis.</p>
<p>In essence, the boxes on the upper right hand side of the whole scatterplot are mirror images of the plots on the lower left hand. In this scatterplot, we can see that there are correlations between price and taxes; year built and taxes; square footage and taxes; because the plot shows a slight slope of the lines. With a slope of zero, we have a horizontal line which indicates that there is no relationship between the variables. We see this with price and acres; square footage and acres; acres and year built; acres and taxes; bedroom count and price; square footage and bathroom count; and in in several other places as well. We also see some vertical lines, which are problematic as they indicate that there is no change in x. Thus our formula is undefined due to division by zero. Some will term this condition infinite slope, but be aware that we can’t tell if it is positive or negative infinity.</p>
<p>Lastly, there are ambiguous or weak correlations between variables. In this case, it would be important to continue further with additional statistical analyses and preferably a larger dataset to confirm or deny any relationships. In essence, we can remove acres, bedrooms and bath count from our logistic regression model. However I must make note that these results could also be as a result of discrete variables being visualized in a scatterplot, so as usual, further analysis is needed.</p>
<p>Sold - Similarly as with active properties, examining the prior relationships between variables is vital, so we can rule out any potential extraneous variables in our model. Looking at the figure underneath, it is apparent that there is far more activity occurring with this dataset (which is expected as the dataset was larger). There also are interesting apparent correlations between price and square footage and taxes and square footage, relationships that were overlooked or not detectable with the active properties dataset. As previously, we see that discrete variables yield vertical slopes and horizontal slopes which are indicative of the absence of a relationship at the moment, but I should mention that there could be the possibility of lurking variables.</p>
<p><img src="graph2.png" alt="second graph"></p>
<p><em>Relationships between the variables of sold properties: We can rule out those variables that appear to have no effect, so we can know what variables will be more significant for the logistic model.</em></p>
<h2 id="The-model"><a href="#The-model" class="headerlink" title="The model"></a>The model</h2><p>I tested the model for multiple predictors along with their interactions.</p>
<p>Above Ground Square Footage + Price + Total Taxes: Testing these multiple predictors to see whether we can reject the null hypothesis (that there is no relationship between the variables) yielded a p-value of 0.00721, which indicates that this is statistically significant and unlikely to have happened purely by chance. For a few prices of the 100,000 range, a single unit change in property taxes increased the log odds of a property being sold by 3.180E-01. For homes where the log odds are sharp, it could be possible that the market price of the home does not coincide with its value. Higher priced properties had a higher log odds of being sold when there was a one unit change in square footage, which is logical.</p>
<p>Another relationship is between taxes and square footage, increasing the log odds of a property being sold as they both change in one unit. The decreases in the log odds of properties being sold were seen with properties in the lower price range with a single unit change in taxes and square footage, -2.945E-04. This suggests that there is a boundary/threshold for square footage/property taxes that should not be crossed if a property is to be sold.</p>
<p>Price + Year Built: For this combination, we see that the null deviance (which is the deviance when x = 0) is 181.944 whereas the residual deviance is 58.669 on 48 degrees of freedom (number of observations - number of predictor values). For every one unit change in the year the property was built, the log odds of the property being sold decreases by -9.907E -04, which signifies that as price and age increase, the odds of a property being sold decrease.</p>
<p>Price + Total Taxes: For every one unit of change of yearly total taxes, the log odds of a property being sold increases by 6.092E-04, but it ultimately depends on the price, as there are wide variations even for similar prices, indicating that there could be some property taxes that surpass the value inferred by market price.</p>
<p>Price + Days on Market: With one unit change in days on market (1 day); the log odds of a property being sold decreases by -4.078E-03. With a lower residual deviance than null, we can say that there is an effect of the days on the market on the property being sold, with those being on the market longer having the odds decrease. Across the board, when this occurred, it appears potential buyers are less likely to want to purchase because with a single unit change in price, there is predominantly a decrease in the odds of selling.</p>
<h2 id="Predicting"><a href="#Predicting" class="headerlink" title="Predicting"></a>Predicting</h2><p>The usefulness of logistic regression modeling is that it can serve as a predictor to gauge the odds that a property will sell given the predictor variables mentioned above. Given the limited nature of the data, I nevertheless decided to run the predictor function created in R: predict(mylogit, type=”response”, newdata=newdata2) to determine whether it would correctly predict the selling of a property that was recently sold, but listed under the data as active.</p>
<p>The result was 1, which predicts that the property does have a significant odd of being sold given the interacting predictor variables. More data would be useful in determining the accuracy of the test data used in the model.</p>
<h2 id="Further-work"><a href="#Further-work" class="headerlink" title="Further work"></a>Further work</h2><p>Further work would require a larger dataset that would allow for more specific filtering the data, allowing for more precise and models with higher accuracy. The ideal method for this filtering would be the algorithm k-nearest neighbor classifier, a popular machine learning technique used for classification. Implemented beforehand with the current dataset, it yielded specific subgroups that shared similar characteristics (for example, data was classified by price and acre of property).</p>
<p>However, given the limited amount of data, certain groups were vastly small in size (approx. 1-2) and odd in dimensions (2 data points for unsold versus 4 data points for sold) which was indicative that the use of this algorithm would be inefficient and likely to yield incorrect results. Given the limited dataset, the method used for dimensionality reduction was not as sophisticated as it could have been, which is something that would be greatly improved with larger datasets. In addition, for greater efficiency of these machine learning algorithms, it is important to have a large training dataset for properties unsold and sold so predictions can be higher in accuracy when working with data of active properties, where one may wish to determine why a property has remained on the market for x period of time.</p>
<p>This brings me to my last suggestion for another avenue for possible further work – having a spread-sheet of data of properties that were recently removed from being active; and to examine the model’s accuracy with it’s predictions. I am currently working on visualizing this data via the JavaScript library <a href="https://d3js.org/" target="_blank" rel="external">d3.js.</a>.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I was recently tasked with discovering some additional information from a set of housing data in the upstate New York region that could help elucidate why some properties sell over others. Given two &lt;code&gt;csv&lt;/code&gt; files with data from active and sold properties, I immediately thought how cool it would be to build a predictive model using logistic regression! I built the whole thing in R, too, which gave me more experience with the software, and surprised me in its simplicity and power. I was limited by my dataset, as it was not as large as I would have hoped for, but nevertheless, I got right down to work.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://.alexsinfarosa.com/tags/Machine-Learning/"/>
    
      <category term="Python" scheme="http://.alexsinfarosa.com/tags/Python/"/>
    
      <category term="Logistic Regression" scheme="http://.alexsinfarosa.com/tags/Logistic-Regression/"/>
    
      <category term="Dimensionality Reduction" scheme="http://.alexsinfarosa.com/tags/Dimensionality-Reduction/"/>
    
  </entry>
  
  <entry>
    <title>Spearman and Pearson Correlation</title>
    <link href="http://.alexsinfarosa.com/2016/05/08/Spearman-and-Pearson-Correlation/"/>
    <id>http://.alexsinfarosa.com/2016/05/08/Spearman-and-Pearson-Correlation/</id>
    <published>2016-05-08T20:53:06.000Z</published>
    <updated>2016-11-18T19:16:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>In the following notebook we will determine whether three sets of numbers are correlated.</p>
<p>We will use one more time the thirty politics articles (those labelled p0.txt through p29.txt) contained in the<br><a href="https://github.com/alexsinfarosa/Baby-Machine-Learning/tree/master/data" target="_blank" rel="external">data</a> folder.</p>
<p>Each of the article webpages has three icons labelling counts for a) Facebook likes, b) Tweets, and c) Comments.</p>
<a id="more"></a>
<h3 id="The-data"><a href="#The-data" class="headerlink" title="The data"></a>The data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">fblikes = [<span class="number">101</span>, <span class="number">54</span>, <span class="number">171</span>, <span class="number">5900</span>, <span class="number">2400</span>, <span class="number">385</span>, <span class="number">14400</span>, <span class="number">356</span>, <span class="number">553</span>, <span class="number">146</span>, <span class="number">14400</span>, <span class="number">359</span>, <span class="number">134</span>, <span class="number">40</span>, <span class="number">3400</span>,</div><div class="line">           <span class="number">933</span>, <span class="number">16800</span>, <span class="number">4400</span>, <span class="number">303</span>, <span class="number">7000</span>, <span class="number">1100</span>, <span class="number">427</span>, <span class="number">214</span>, <span class="number">94</span>, <span class="number">1300</span>, <span class="number">1800</span>, <span class="number">882</span>, <span class="number">90</span>, <span class="number">2800</span>, <span class="number">2400</span>]</div><div class="line">tweets = [<span class="number">48</span>, <span class="number">29</span>, <span class="number">67</span>, <span class="number">950</span>, <span class="number">201</span>, <span class="number">103</span>, <span class="number">757</span>, <span class="number">108</span>, <span class="number">130</span>, <span class="number">41</span>, <span class="number">901</span>, <span class="number">80</span>, <span class="number">63</span>, <span class="number">43</span>, <span class="number">205</span>,</div><div class="line">          <span class="number">95</span>, <span class="number">432</span>, <span class="number">260</span>, <span class="number">127</span>, <span class="number">326</span>, <span class="number">75</span>, <span class="number">108</span>, <span class="number">60</span>, <span class="number">52</span>, <span class="number">147</span>, <span class="number">228</span>, <span class="number">75</span>, <span class="number">51</span>, <span class="number">291</span>, <span class="number">761</span>]</div><div class="line">comments = [<span class="number">274</span>, <span class="number">290</span>, <span class="number">98</span>, <span class="number">546</span>, <span class="number">111</span>, <span class="number">185</span>, <span class="number">1900</span>, <span class="number">108</span>, <span class="number">563</span>, <span class="number">158</span>, <span class="number">934</span>, <span class="number">1100</span>, <span class="number">885</span>, <span class="number">122</span>, <span class="number">166</span>,</div><div class="line">           <span class="number">332</span>, <span class="number">1700</span>, <span class="number">1800</span>, <span class="number">213</span>, <span class="number">2300</span>, <span class="number">3000</span>, <span class="number">11</span>, <span class="number">63</span>, <span class="number">107</span>, <span class="number">142</span>, <span class="number">466</span>, <span class="number">327</span>, <span class="number">49</span>, <span class="number">2000</span>, <span class="number">194</span>]</div></pre></td></tr></table></figure>
<h3 id="Importing-Libraries"><a href="#Importing-Libraries" class="headerlink" title="Importing Libraries"></a>Importing Libraries</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr,spearmanr,linregress</div><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div></pre></td></tr></table></figure>
<h3 id="Determine-correlation-between-the-sets"><a href="#Determine-correlation-between-the-sets" class="headerlink" title="Determine correlation between the sets"></a>Determine correlation between the sets</h3><p>For the three pairwise possibilities, we will check using <code>pearsonr()</code> and <code>spearmanr()</code> (both from <code>scipy.stats</code>) whether they are positively or negatively correlated with less than <code>p=.05</code> (meaning that there’s less than a 5% probability that the observed correlation would be seen in randomized data).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> pair,c1,c2 <span class="keyword">in</span> (<span class="string">'fb/tw'</span>,fblikes,tweets),(<span class="string">'fb/co'</span>,fblikes,comments),(<span class="string">'tw/co'</span>,tweets,comments):</div><div class="line">    r,p=pearsonr(c1,c2)</div><div class="line">    print(pair,<span class="string">'Pearson  r=&#123;:.2f&#125;, p=&#123;:.6f&#125;'</span>.format(r,p))</div><div class="line">    r,p=spearmanr(c1,c2)</div><div class="line">    print(<span class="string">'      Spearman r=&#123;:.2f&#125;, p=&#123;:.6f&#125;'</span>.format(r,p))</div></pre></td></tr></table></figure>
<pre><code>fb/tw Pearson  r=0.74, p=0.000002
      Spearman r=0.93, p=0.000000
fb/co Pearson  r=0.50, p=0.004842
      Spearman r=0.58, p=0.000735
tw/co Pearson  r=0.27, p=0.142944
      Spearman r=0.47, p=0.008601
</code></pre><p>The results above show that they are all positively correlated (with the Spearman correlation between facebook likes and tweets particularly strong with <code>r=.93</code>). Only the Pearson <code>tw/co</code> correlation is not statistically significant at the <code>p=.05</code> level.</p>
<p>We also notice from the results that the pair <code>tw/co</code> has a statistically significant Spearman, but not Pearson, correlation. Let’s try to graph it to get some intuition.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">plt.plot(comments,tweets,<span class="string">'ro'</span>,alpha=<span class="number">.3</span>)</div><div class="line">plt.xlabel(<span class="string">'tweets'</span>)</div><div class="line">plt.ylabel(<span class="string">'comments'</span>)</div><div class="line">plt.grid(<span class="string">'on'</span>);</div></pre></td></tr></table></figure>
<p><img src="output_9_0.png" alt="png"></p>
<p>Pearson was not able to see the linear regression because the data is spread out.</p>
<p>Now, for each word, let’s determine whether its number counts in the thirty articles have a statistically significant correlation (again at the less than <code>p =.05</code> level) with each of the three count lists, using the two types of correlation. We’ll just print a list of words, giving the value of the Pearson or Spearman correlation, or both, if statistically significant.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">testwords = [<span class="string">'about'</span>, <span class="string">'are'</span>, <span class="string">'been'</span>, <span class="string">'bush'</span>, <span class="string">'candidate'</span>, <span class="string">'cruz'</span>, <span class="string">'hampshire'</span>, <span class="string">'has'</span>,</div><div class="line">             <span class="string">'have'</span>, <span class="string">'her'</span>, <span class="string">'new'</span>, <span class="string">'obama'</span>, <span class="string">'out'</span>, <span class="string">'party'</span>, <span class="string">'president'</span>, <span class="string">'she'</span>,</div><div class="line">             <span class="string">'state'</span>, <span class="string">'the'</span>, <span class="string">'this'</span>, <span class="string">'was'</span>, <span class="string">'were'</span>, <span class="string">'which'</span>, <span class="string">'will'</span>, <span class="string">'win'</span>]</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ptexts=[open(<span class="string">'data/p&#123;&#125;.txt'</span>.format(i)).read() <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">30</span>)]</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</div><div class="line"></div><div class="line">wordcounter = [Counter(re.findall(<span class="string">'[a-z]+'</span>, ptexts[i].lower())) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(ptexts))]</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">w_test</span><span class="params">(w)</span>:</span></div><div class="line">    counts=[c[w] <span class="keyword">for</span> c <span class="keyword">in</span> wordcounter]</div><div class="line">    <span class="keyword">for</span> clist,lab <span class="keyword">in</span> zip((fblikes,tweets,comments),(<span class="string">'fb'</span>,<span class="string">'tw'</span>,<span class="string">'co'</span>)):</div><div class="line">        r,p=pearsonr(counts,clist)</div><div class="line">        <span class="keyword">if</span> p&lt;<span class="number">.05</span>: print(<span class="string">'&#123;&#125;/&#123;&#125; Pearson  r=&#123;:.2f&#125;, p=&#123;:.6f&#125;'</span>.format(w,lab,r,p))</div><div class="line">        r,p=spearmanr(counts,clist)</div><div class="line">        <span class="keyword">if</span> p&lt;<span class="number">.05</span>: print(<span class="string">'&#123;&#125;/&#123;&#125; Spearman r=&#123;:.2f&#125;, p=&#123;:.6f&#125;'</span>.format(w,lab,r,p))</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> w <span class="keyword">in</span> testwords: w_test(w)</div></pre></td></tr></table></figure>
<pre><code>about/fb Pearson  r=0.37, p=0.043212
about/fb Spearman r=0.40, p=0.030467
about/tw Pearson  r=0.54, p=0.002213
about/tw Spearman r=0.52, p=0.003306
are/tw Spearman r=-0.39, p=0.034086
been/fb Spearman r=-0.40, p=0.030489
been/tw Spearman r=-0.37, p=0.042871
bush/fb Spearman r=-0.36, p=0.048523
bush/co Spearman r=-0.38, p=0.037912
candidate/fb Spearman r=-0.51, p=0.004016
candidate/tw Pearson  r=-0.36, p=0.049033
candidate/tw Spearman r=-0.46, p=0.009707
cruz/tw Pearson  r=0.46, p=0.011164
hampshire/fb Pearson  r=-0.38, p=0.038373
hampshire/fb Spearman r=-0.58, p=0.000763
hampshire/tw Pearson  r=-0.41, p=0.025318
hampshire/tw Spearman r=-0.65, p=0.000108
has/fb Spearman r=-0.37, p=0.045807
have/co Spearman r=0.45, p=0.012104
her/co Pearson  r=0.51, p=0.003629
new/fb Pearson  r=-0.40, p=0.028577
new/fb Spearman r=-0.60, p=0.000492
new/tw Pearson  r=-0.47, p=0.009203
new/tw Spearman r=-0.72, p=0.000007
obama/co Pearson  r=0.41, p=0.023994
out/fb Spearman r=-0.39, p=0.034711
party/tw Pearson  r=-0.40, p=0.030355
president/fb Pearson  r=0.47, p=0.008472
president/co Pearson  r=0.39, p=0.031265
she/co Pearson  r=0.63, p=0.000165
state/tw Spearman r=-0.48, p=0.006972
the/tw Pearson  r=0.38, p=0.038402
the/tw Spearman r=0.39, p=0.033835
this/fb Pearson  r=0.44, p=0.015278
this/fb Spearman r=0.60, p=0.000431
this/tw Pearson  r=0.43, p=0.018816
this/tw Spearman r=0.60, p=0.000416
this/co Spearman r=0.45, p=0.011906
was/fb Pearson  r=0.40, p=0.027266
was/fb Spearman r=0.40, p=0.029785
were/tw Spearman r=0.42, p=0.020235
which/tw Pearson  r=0.46, p=0.010975
will/fb Spearman r=-0.39, p=0.031619
win/fb Pearson  r=-0.38, p=0.037361
win/fb Spearman r=-0.56, p=0.001195
win/tw Spearman r=-0.44, p=0.014782
</code></pre><h3 id="Find-most-correlated-words"><a href="#Find-most-correlated-words" class="headerlink" title="Find most correlated words"></a>Find most correlated words</h3><p>Lastly, we will find words from the above tests that have the most positive and most negative Pearson correlations between number counts and each of the three lists.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">plist=&#123;<span class="string">'fb'</span>:[],<span class="string">'tw'</span>:[],<span class="string">'co'</span>:[]&#125;</div><div class="line"><span class="keyword">for</span> w <span class="keyword">in</span> testwords:</div><div class="line">    counts=[c[w] <span class="keyword">for</span> c <span class="keyword">in</span> wordcounter]</div><div class="line">    <span class="keyword">for</span> clist,lab <span class="keyword">in</span> zip((fblikes,tweets,comments),(<span class="string">'fb'</span>,<span class="string">'tw'</span>,<span class="string">'co'</span>)):</div><div class="line">        r,p=pearsonr(counts,clist)</div><div class="line">        <span class="keyword">if</span> p&lt;<span class="number">.05</span>: plist[lab].append((w,r,p))</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> m <span class="keyword">in</span> max,min:</div><div class="line">   <span class="keyword">for</span> lab <span class="keyword">in</span> (<span class="string">'fb'</span>,<span class="string">'tw'</span>,<span class="string">'co'</span>):</div><div class="line">       print(lab,m(plist[lab],key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>]))</div><div class="line">   print()</div></pre></td></tr></table></figure>
<pre><code>fb (&apos;president&apos;, 0.47188160505044835, 0.0084721575440603516)
tw (&apos;about&apos;, 0.53706047291383308, 0.0022125814879941463)
co (&apos;she&apos;, 0.63480048053090932, 0.00016457778749341651)

fb (&apos;new&apos;, -0.39985352242322153, 0.02857665659532841)
tw (&apos;new&apos;, -0.46742494363223464, 0.0092029490096462545)
co (&apos;president&apos;, 0.39389143259482368, 0.031264888900746315)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">view</span><span class="params">(w,clist,ylab=<span class="string">''</span>)</span>:</span></div><div class="line">    counts=[c[w] <span class="keyword">for</span> c <span class="keyword">in</span> wordcounter]</div><div class="line">    plt.plot(counts,clist,<span class="string">'o'</span>,alpha=<span class="number">.33</span>)</div><div class="line">    lr=linregress(counts,clist)</div><div class="line">    xr=np.arange(max(counts)+<span class="number">1</span>)</div><div class="line">    plt.plot(xr,lr[<span class="number">0</span>]*xr+lr[<span class="number">1</span>],label=<span class="string">'slope=&#123;:.2f&#125;'</span>.format(lr[<span class="number">0</span>]))</div><div class="line">    plt.xlabel(<span class="string">'#occurrences of word "&#123;&#125;"'</span>.format(w))</div><div class="line">    plt.title(<span class="string">'slope=&#123;:.2f&#125;'</span>.format(lr[<span class="number">0</span>]))</div><div class="line">    plt.ylabel(ylab)</div><div class="line">    plt.xlim(<span class="number">-.5</span>,max(counts)+<span class="number">.5</span>)</div><div class="line">    plt.ylim(<span class="number">-.05</span>*max(clist),<span class="number">1.05</span>*max(clist))</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">i=<span class="number">0</span></div><div class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">7</span>))</div><div class="line"><span class="keyword">for</span> w,clist,lab <span class="keyword">in</span> zip((<span class="string">'president'</span>,<span class="string">'about'</span>,<span class="string">'she'</span>),(fblikes,tweets,comments),(<span class="string">'fb'</span>,<span class="string">'tw'</span>,<span class="string">'co'</span>)):</div><div class="line">    i+=<span class="number">1</span></div><div class="line">    plt.subplot(<span class="number">2</span>,<span class="number">3</span>,i)</div><div class="line">    view(w,clist,lab)</div><div class="line"><span class="keyword">for</span> w,clist,lab <span class="keyword">in</span> zip((<span class="string">'new'</span>,<span class="string">'new'</span>,<span class="string">'president'</span>),(fblikes,tweets,comments),(<span class="string">'fb'</span>,<span class="string">'tw'</span>,<span class="string">'co'</span>)):</div><div class="line">    i+=<span class="number">1</span></div><div class="line">    plt.subplot(<span class="number">2</span>,<span class="number">3</span>,i)</div><div class="line">    view(w,clist,lab)</div><div class="line">plt.tight_layout()</div></pre></td></tr></table></figure>
<p><img src="output_22_0.png" alt="png"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In the following notebook we will determine whether three sets of numbers are correlated.&lt;/p&gt;
&lt;p&gt;We will use one more time the thirty politics articles (those labelled p0.txt through p29.txt) contained in the&lt;br&gt;&lt;a href=&quot;https://github.com/alexsinfarosa/Baby-Machine-Learning/tree/master/data&quot;&gt;data&lt;/a&gt; folder.&lt;/p&gt;
&lt;p&gt;Each of the article webpages has three icons labelling counts for a) Facebook likes, b) Tweets, and c) Comments.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://.alexsinfarosa.com/tags/Machine-Learning/"/>
    
      <category term="Python" scheme="http://.alexsinfarosa.com/tags/Python/"/>
    
      <category term="Spearman Correlation" scheme="http://.alexsinfarosa.com/tags/Spearman-Correlation/"/>
    
      <category term="Pearson Correlation" scheme="http://.alexsinfarosa.com/tags/Pearson-Correlation/"/>
    
      <category term="Linear Regression" scheme="http://.alexsinfarosa.com/tags/Linear-Regression/"/>
    
  </entry>
  
  <entry>
    <title>Logistic Regression</title>
    <link href="http://.alexsinfarosa.com/2016/05/04/Logistic-Regression/"/>
    <id>http://.alexsinfarosa.com/2016/05/04/Logistic-Regression/</id>
    <published>2016-05-04T15:49:06.000Z</published>
    <updated>2016-09-30T15:00:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>This time we are going to look at logistic regression. The data were obtained by the<br><a href="https://en.wikipedia.org/wiki/Space_Shuttle_Challenger_disaster" target="_blank" rel="external">Space Shuttle Challenger disaster</a> O-ring data failure data (from 1986):</p>
<a id="more"></a>
<p>Failures at temperatures: [70, 57, 63, 70, 53, 75, 58]</p>
<p>Successes at temperatures: [66, 69, 68, 67, 72, 73, 70, 78, 67, 67, 75, 70, 81, 76, 79, 76]</p>
<p>Apparently they were aware of failures and successes at various temperatures, but were not considering the temperature dependence of these numbers when making the decision to launch at $31^\circ$ F.</p>
<h3 id="Importing-libraries"><a href="#Importing-libraries" class="headerlink" title="Importing libraries"></a>Importing libraries</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div></pre></td></tr></table></figure>
<h3 id="Creating-lists-to-work-with-the-data"><a href="#Creating-lists-to-work-with-the-data" class="headerlink" title="Creating lists to work with the data"></a>Creating lists to work with the data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">failure=[<span class="number">70</span>, <span class="number">57</span>, <span class="number">63</span>, <span class="number">70</span>, <span class="number">53</span>, <span class="number">75</span>, <span class="number">58</span>]</div><div class="line">success=[<span class="number">66</span>, <span class="number">69</span>, <span class="number">68</span>, <span class="number">67</span>, <span class="number">72</span>, <span class="number">73</span>, <span class="number">70</span>, <span class="number">78</span>, <span class="number">67</span>, <span class="number">67</span>, <span class="number">75</span>, <span class="number">70</span>, <span class="number">81</span>, <span class="number">76</span>, <span class="number">79</span>, <span class="number">76</span>]</div><div class="line">xdata,ydata=zip(*[[t,<span class="number">0</span>] <span class="keyword">for</span> t <span class="keyword">in</span> failure] + [[t,<span class="number">1</span>] <span class="keyword">for</span> t <span class="keyword">in</span> success])</div></pre></td></tr></table></figure>
<h3 id="Plotting-the-data"><a href="#Plotting-the-data" class="headerlink" title="Plotting the data"></a>Plotting the data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">plt.figure(figsize=(<span class="number">12.5</span>, <span class="number">4</span>))</div><div class="line">plt.yticks(np.arange(<span class="number">0</span>,<span class="number">1.01</span>,<span class="number">.2</span>))</div><div class="line">plt.plot(xdata,ydata,<span class="string">'ko'</span>,alpha=<span class="number">.33</span>,markersize=<span class="number">8</span>)</div><div class="line">plt.ylim(<span class="number">-.05</span>,<span class="number">1.05</span>)</div><div class="line">plt.grid(<span class="string">'on'</span>)</div></pre></td></tr></table></figure>
<p><img src="output_6_0.png" alt="png"></p>
<p>Looking at the above graph of this data, suggests modeling the data with a logistic function:<br>Assuming the probability of success at temperature $t$ is given by $p(t) = 1/(1+\exp(-a(t-t_0))$, let’s find the maximum likelihood fit for the values of $a$ and $t_0$.</p>
<p>For these values, what is the probability of failure at the actual launch temperature $t=31^\circ$ F.?</p>
<p>Note that the transparency alpha=.33 makes visible when points are on top of each other (e.g., there are three success data points for t=67, and two failures and two successes for t=70).</p>
<h3 id="Define-and-plot-the-logistic-function"><a href="#Define-and-plot-the-logistic-function" class="headerlink" title="Define and plot the logistic function"></a>Define and plot the logistic function</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">p</span><span class="params">(t,a=<span class="number">1.</span>,t0=<span class="number">0.</span>)</span>:</span> <span class="keyword">return</span> <span class="number">1.</span>/(<span class="number">1.</span>+np.exp(-a*(t-t0)))</div><div class="line"></div><div class="line">plt.figure(figsize=(<span class="number">12.5</span>, <span class="number">4</span>))</div><div class="line">plt.xlim(<span class="number">-7</span>,<span class="number">7</span>)</div><div class="line">xr=np.arange(<span class="number">-7.</span>,<span class="number">7.1</span>,<span class="number">.1</span>)</div><div class="line">plt.plot(xr,p(xr),<span class="string">'r--'</span>);</div></pre></td></tr></table></figure>
<p><img src="output_9_0.png" alt="png"></p>
<h3 id="Objective"><a href="#Objective" class="headerlink" title="Objective"></a>Objective</h3><p>The object is to find parameters <code>a</code> and <code>t0</code> for the logistic <code>p(t,a,t0)</code> that give the highest overall probability of the data.</p>
<p>For each value of <code>t0</code> and <code>a</code>, there’s a sigmoid function <code>p(t,a,t0)</code>. Mathematically that’s $p(t\ |\ t_0, a)$,<br>the probability that the O-ring is OK at temperature <code>t</code>, given some value of <code>t0</code> and <code>a</code>.<br>The probability of a data point is therefore <code>p(t,a,t0)</code> if the datapoint is a success (1), and <code>1- p(t, t0, a)</code> if the datapoint is a failure (0).</p>
<p>To calculate the overall probability of the data (for some fixed <code>t0,a</code>), we take a product of the above probabilities over the 23 data points (7 failures, 16 successes), i.e.,<br><code>(1-p(53 , t0,a)) * (1- p(57 , t0, a)) * ... * p(79 , t0, a) * p(78 , t0, a)</code><br>(and if a datapoint occurs more than once, it is included multiple times in the product, according to how it occurred each time).</p>
<p>The optimal values of <code>t0</code> and <code>a</code> maximize the probability of the data (given the assumption that the probability of failure is given by a sigmoid function in the first place).</p>
<p>Then for those maximum probable values, we will calculate the probability of failure for <code>t=31</code>.</p>
<h4 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h4><p>It’s usually preferable to calculate the log of something small like a product of probabilities to avoid underflow. The log of the above expression is just the sum of the logs of the terms in the product, so the result would be<br><code>np.sum(np.log(1-p(T0,t0,a))) + np.sum (np.log(p(T1,t0,a))</code><br>where <code>T0</code> is an array of values of t with failed o-ring (0), and T1 is an array of values of t with success (1).<br><br>Note that <code>p()</code> of an array will give a list of probabilities for each value in the array, then <code>np.log()</code> will similarly map onto each term on the list, then those values are summed by <code>np.sum()</code>.</p>
<p>There are more efficient ways of finding optimal values of parameters, but since there are only two parameters in this case it is possible to perform a brute-force search over the 2-dimensional grid of  values of <code>a in np.arange(.01,1.,.01)</code> and <code>t0 in np.arange(50,85.001,.5)</code>. That would be 100 values of <code>a</code> from .01 to 1, and 72 values of <code>t0</code> from 50 to 85.</p>
<h3 id="Finding-the-max"><a href="#Finding-the-max" class="headerlink" title="Finding the max"></a>Finding the max</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">p_max= &#123;&#125;</div><div class="line"><span class="keyword">for</span> a <span class="keyword">in</span> np.arange(<span class="number">.01</span>,<span class="number">1.</span>,<span class="number">.01</span>):</div><div class="line">    <span class="keyword">for</span> t0 <span class="keyword">in</span> np.arange(<span class="number">50</span>,<span class="number">85.001</span>,<span class="number">.5</span>):</div><div class="line">        p_max[a,t0] = np.sum(np.log(<span class="number">1</span>-p(failure,a,t0))) + np.sum(np.log(p(success,a,t0)))</div><div class="line">max(p_max.values())</div></pre></td></tr></table></figure>
<pre><code>-10.161118065421583
</code></pre><h3 id="Find-optimal-values-of-a-and-t0"><a href="#Find-optimal-values-of-a-and-t0" class="headerlink" title="Find optimal values of  a and t0"></a>Find optimal values of  <code>a</code> and <code>t0</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> p_max.items():</div><div class="line">    <span class="keyword">if</span> v == max(p_max.values()):</div><div class="line">        print(<span class="string">'max at a=&#123;0&#125;, t0=&#123;1&#125;'</span>.format(k[<span class="number">0</span>],k[<span class="number">1</span>]))</div></pre></td></tr></table></figure>
<pre><code>max at a=0.24000000000000002, t0=65.0
</code></pre><h3 id="Plot-the-resulting-logistic-function-on-the-original-graph"><a href="#Plot-the-resulting-logistic-function-on-the-original-graph" class="headerlink" title="Plot the resulting logistic function on the original graph"></a>Plot the resulting logistic function on the original graph</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">plt.figure(figsize=(<span class="number">12.5</span>, <span class="number">4</span>))</div><div class="line">plt.yticks(np.arange(<span class="number">0</span>,<span class="number">1.01</span>,<span class="number">.2</span>))</div><div class="line">plt.ylim(<span class="number">-.05</span>,<span class="number">1.05</span>)</div><div class="line">xr=np.arange(<span class="number">50</span>,<span class="number">85.001</span>,<span class="number">.5</span>)</div><div class="line"></div><div class="line">plt.plot(xdata,ydata,<span class="string">'ko'</span>,alpha=<span class="number">.33</span>,markersize=<span class="number">8</span>)</div><div class="line">plt.plot(xr,p(xr,<span class="number">0.24</span>,<span class="number">65</span>),<span class="string">'b--'</span>)</div><div class="line">plt.legend([<span class="string">'data'</span>,<span class="string">'max likelihood\nlogistic fit\n a=.24, t0=65'</span>],loc=<span class="string">'center right'</span>);</div><div class="line"></div><div class="line">plt.grid(<span class="string">'on'</span>)</div></pre></td></tr></table></figure>
<p><img src="output_18_0.png" alt="png"></p>
<h3 id="Should-we-launch"><a href="#Should-we-launch" class="headerlink" title="Should we launch?"></a>Should we launch?</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">print(<span class="string">'probability of failure at t=31 = &#123;:.2%&#125;'</span>.format(<span class="number">1</span>-p(<span class="number">31</span>,<span class="number">.24</span>,<span class="number">65</span>)))</div></pre></td></tr></table></figure>
<pre><code>probability of failure at t=31 = 99.97%
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This time we are going to look at logistic regression. The data were obtained by the&lt;br&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Space_Shuttle_Challenger_disaster&quot;&gt;Space Shuttle Challenger disaster&lt;/a&gt; O-ring data failure data (from 1986):&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://.alexsinfarosa.com/tags/Machine-Learning/"/>
    
      <category term="Python" scheme="http://.alexsinfarosa.com/tags/Python/"/>
    
      <category term="Logistic Regression" scheme="http://.alexsinfarosa.com/tags/Logistic-Regression/"/>
    
  </entry>
  
  <entry>
    <title>Simple Spelling Corrector</title>
    <link href="http://.alexsinfarosa.com/2016/04/23/Simple-Spelling-Corrector/"/>
    <id>http://.alexsinfarosa.com/2016/04/23/Simple-Spelling-Corrector/</id>
    <published>2016-04-23T15:45:29.000Z</published>
    <updated>2016-09-27T21:48:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>In “How to Write a Spelling Corrector” <a href="http://norvig.com/spell-correct.html" target="_blank" rel="external">http://norvig.com/spell-correct.html</a>, Norvig describes training his algorithm on about a million words taken from several public domain books. Various precompiled word frequency lists are also available on the web, for example for all of the Project Gutenberg texts up to Apr 2006 at <a href="http://en.wiktionary.org/wiki/Wiktionary:Frequency_lists#Project_Gutenberg" target="_blank" rel="external">http://en.wiktionary.org/wiki/Wiktionary:Frequency_lists#Project_Gutenberg</a> .</p>
<a id="more"></a>
<p>From that list, the frequencies per billion for the following selection of twenty four words are roughly</p>
<table><br><tr><td>fog 13,651 </td><td>for 7,097,981 </td><td>ford 15,620 </td><td>fore 6,699 </td><td>forge 3,893 </td><td>fork  8,219</td></tr><br><tr><br><td>form 307,506<br></td><td>fort 23,113<br></td><td>frog 4,119<br></td><td>go 816,536<br></td><td>god 552,668<br></td><td>good 966,602<br></td></tr><tr><br><td>gorge  6,177<br></td><td>got 432,016<br></td><td>groan 9,995<br></td><td>grow 64,005<br></td><td>grown 57,772<br></td><td>lord 422,407<br></td></tr><tr><br><td>more 1,899,787<br></td><td>nor 349,691<br></td><td>of 33,950,064<br></td><td>off 545,832<br></td><td>or 4,228,287<br></td><td>work 829,823<br></td></tr><br></table>


<p>Now, we would like to write a function <code>correct()</code> that returns the suggested corrected spelling for each of the four words: <strong>‘frog’</strong>, <strong>‘gorf’</strong>, <strong>‘forg’</strong>, and <strong>‘grof’</strong>.</p>
<h2 id="Bring-in-the-table…"><a href="#Bring-in-the-table…" class="headerlink" title="Bring in the table…"></a>Bring in the table…</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">table = &#123;<span class="string">"fog"</span>:<span class="number">13651</span>,<span class="string">"for"</span>:<span class="number">7097981</span>,<span class="string">"ford"</span>:<span class="number">15620</span>,<span class="string">"fore"</span>:<span class="number">6699</span>,<span class="string">"forge"</span>:<span class="number">3893</span>,<span class="string">"fork"</span>:<span class="number">8219</span>,<span class="string">"form"</span>:<span class="number">307506</span>,<span class="string">"fort"</span>:<span class="number">23113</span>,</div><div class="line">        <span class="string">"frog"</span>:<span class="number">4119</span>,<span class="string">"go"</span>:<span class="number">816536</span>,<span class="string">"god"</span>:<span class="number">552668</span>,<span class="string">"good"</span>:<span class="number">966602</span>,<span class="string">"gorge"</span>:<span class="number">6177</span>,<span class="string">"got"</span>:<span class="number">432016</span>,<span class="string">"groan"</span>:<span class="number">9995</span>,</div><div class="line">        <span class="string">"grow"</span>:<span class="number">64005</span>,<span class="string">"grown"</span>:<span class="number">57772</span>,<span class="string">"lord"</span>:<span class="number">422407</span>,<span class="string">"more"</span>:<span class="number">1899787</span>,<span class="string">"nor"</span>:<span class="number">349691</span>,<span class="string">"of"</span>:<span class="number">33950064</span>,<span class="string">"off"</span>:<span class="number">545832</span>,<span class="string">"or"</span>:<span class="number">4228287</span>,</div><div class="line">        <span class="string">"work"</span>:<span class="number">829823</span>&#125;</div></pre></td></tr></table></figure>
<p>Let’s get the functions <code>edits1</code> and <code>edits2</code> from Norvig’s article.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">edits1</span><span class="params">(word)</span>:</span></div><div class="line">    <span class="string">"All edits that are one edit away from `word`."</span></div><div class="line">    alphabet = <span class="string">'abcdefghijklmnopqrstuvwxyz'</span></div><div class="line">    splits     = [(word[:i], word[i:]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word) + <span class="number">1</span>)]</div><div class="line">    deletes    = [a + b[<span class="number">1</span>:] <span class="keyword">for</span> a, b <span class="keyword">in</span> splits <span class="keyword">if</span> b]</div><div class="line">    transposes = [a + b[<span class="number">1</span>] + b[<span class="number">0</span>] + b[<span class="number">2</span>:] <span class="keyword">for</span> a, b <span class="keyword">in</span> splits <span class="keyword">if</span> len(b)&gt;<span class="number">1</span>]</div><div class="line">    replaces   = [a + c + b[<span class="number">1</span>:] <span class="keyword">for</span> a, b <span class="keyword">in</span> splits <span class="keyword">for</span> c <span class="keyword">in</span> alphabet <span class="keyword">if</span> b]</div><div class="line">    inserts    = [a + c + b <span class="keyword">for</span> a, b <span class="keyword">in</span> splits <span class="keyword">for</span> c <span class="keyword">in</span> alphabet]</div><div class="line">    <span class="keyword">return</span> set(deletes + transposes + replaces + inserts)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">edits2</span><span class="params">(word)</span>:</span></div><div class="line">    <span class="string">"All edits that are two edits away from `word`."</span></div><div class="line">    <span class="keyword">return</span> set(e2 <span class="keyword">for</span> e1 <span class="keyword">in</span> edits1(word) <span class="keyword">for</span> e2 <span class="keyword">in</span> edits1(e1) <span class="keyword">if</span> e2 <span class="keyword">in</span> table)</div></pre></td></tr></table></figure>
<p>Write a function that returns the maximum frequency candidate at edit distance 1.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">d1</span><span class="params">(word)</span>:</span></div><div class="line">    arr = &#123;&#125;</div><div class="line">    d1 = edits1(word)</div><div class="line">    result = <span class="string">"None"</span></div><div class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> d1:</div><div class="line">        <span class="keyword">if</span> w <span class="keyword">in</span> table.keys():</div><div class="line">            arr[w] = table[w]</div><div class="line">            result = max(arr, key=arr.get)</div><div class="line">    <span class="keyword">return</span> result</div></pre></td></tr></table></figure>
<p>If the candidate is not found at edit distance 1, then try edit distance 2.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">d2</span><span class="params">(word)</span>:</span></div><div class="line">    arr = &#123;&#125;</div><div class="line">    d2 = edits2(word)</div><div class="line">    result = <span class="string">"None"</span></div><div class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> d2:</div><div class="line">        <span class="keyword">if</span> w <span class="keyword">in</span> table.keys():</div><div class="line">            arr[w] = table[w]</div><div class="line">            result = max(arr, key=arr.get)</div><div class="line">    <span class="keyword">return</span> result</div></pre></td></tr></table></figure>
<p>Bring it all together</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">correct</span><span class="params">(table,list)</span>:</span></div><div class="line">    result = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> list:</div><div class="line">        <span class="keyword">if</span> w <span class="keyword">in</span> table:</div><div class="line">            result[w] = w</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            result[w] = d1(w)</div><div class="line">            <span class="keyword">if</span> result[w] == <span class="string">"None"</span>:</div><div class="line">                result[w] = d2(w)</div><div class="line">    <span class="keyword">return</span> result</div></pre></td></tr></table></figure>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">words_to_check = [<span class="string">'frog'</span>,<span class="string">'gorf'</span>,<span class="string">'forg'</span>,<span class="string">'grof'</span>]</div><div class="line"></div><div class="line">correct(table,words_to_check)</div></pre></td></tr></table></figure>
<pre><code>{&apos;forg&apos;: &apos;for&apos;, &apos;frog&apos;: &apos;frog&apos;, &apos;gorf&apos;: &apos;of&apos;, &apos;grof&apos;: &apos;grow&apos;}
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In “How to Write a Spelling Corrector” &lt;a href=&quot;http://norvig.com/spell-correct.html&quot;&gt;http://norvig.com/spell-correct.html&lt;/a&gt;, Norvig describes training his algorithm on about a million words taken from several public domain books. Various precompiled word frequency lists are also available on the web, for example for all of the Project Gutenberg texts up to Apr 2006 at &lt;a href=&quot;http://en.wiktionary.org/wiki/Wiktionary:Frequency_lists#Project_Gutenberg&quot;&gt;http://en.wiktionary.org/wiki/Wiktionary:Frequency_lists#Project_Gutenberg&lt;/a&gt; .&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://.alexsinfarosa.com/tags/Machine-Learning/"/>
    
      <category term="Python" scheme="http://.alexsinfarosa.com/tags/Python/"/>
    
      <category term="Spelling Corrector" scheme="http://.alexsinfarosa.com/tags/Spelling-Corrector/"/>
    
      <category term="Peter Norvig" scheme="http://.alexsinfarosa.com/tags/Peter-Norvig/"/>
    
  </entry>
  
  <entry>
    <title>Zipf&#39;s Law</title>
    <link href="http://.alexsinfarosa.com/2016/03/16/Zipf-s-Law/"/>
    <id>http://.alexsinfarosa.com/2016/03/16/Zipf-s-Law/</id>
    <published>2016-03-16T16:02:02.000Z</published>
    <updated>2016-09-30T15:11:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>The linguist George Kingsley Zipf, who theorised that given a large body of language (that is, a long book — or every word uttered by Plus employees during the day), the frequency of each word is close to inversely proportional to its rank in the frequency table.</p>
<p>In this guide we’ll write some code to look at some word distributions and power laws.</p>
<a id="more"></a>
<h2 id="The-data"><a href="#The-data" class="headerlink" title="The data"></a>The data</h2><p>The <a href="https://github.com/alexsinfarosa/Baby-Machine-Learning/tree/master/data" target="_blank" rel="external">data</a> folder contains 71 files:</p>
<ul>
<li>Thirty files labelled p0.txt – p29.txt, with text extracted from ten recent news articles about politics.</li>
<li>Thirty files labelled s0.txt – s29.txt, with text extracted from ten recent news articles about sports.</li>
<li>Ten files labelled t0.txt – t9.txt, which will be the “test set”.</li>
</ul>
<p>First, let’s import the texts themselves into our notebook.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ptexts=[open(<span class="string">'data/p&#123;&#125;.txt'</span>.format(i)).read() <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">30</span>)]</div><div class="line">stexts=[open(<span class="string">'data/s&#123;&#125;.txt'</span>.format(i)).read() <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">30</span>)]</div><div class="line">ttexts=[open(<span class="string">'data/t&#123;&#125;.txt'</span>.format(i)).read() <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)]</div></pre></td></tr></table></figure>
<p>The following code iterates through the sixty training texts, and accumulates their word counts in the Counter object <code>thewords</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">%pylab inline</div><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</div><div class="line"></div><div class="line">thewords=Counter()</div><div class="line"><span class="keyword">for</span> txt <span class="keyword">in</span> ptexts+stexts:</div><div class="line">    thewords += Counter(txt.lower().split())</div></pre></td></tr></table></figure>
<p>For instance, we could look at the 10 most common words in the Counter object <code>thewords</code> by using the <code>most_common</code> method as follows:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">thewords.most_common(<span class="number">10</span>)</div></pre></td></tr></table></figure>
<pre><code>[(&apos;the&apos;, 3692),
 (&apos;to&apos;, 1688),
 (&apos;a&apos;, 1665),
 (&apos;of&apos;, 1578),
 (&apos;and&apos;, 1478),
 (&apos;in&apos;, 1223),
 (&apos;that&apos;, 921),
 (&apos;is&apos;, 644),
 (&apos;for&apos;, 625),
 (&apos;he&apos;, 612)]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">words,nw=zip(*thewords.most_common())</div></pre></td></tr></table></figure>
<p>Total number of words in the corpus:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">corpus = sum(nw)</div><div class="line">print(corpus)</div></pre></td></tr></table></figure>
<pre><code>63827
</code></pre><h2 id="Rank-frequency-graph"><a href="#Rank-frequency-graph" class="headerlink" title="Rank frequency graph"></a>Rank frequency graph</h2><p>The frequency of occurrence vs. the rank of the word.</p>
<p>Now we plot the number of occurrences in the <code>thewords</code> dictionary on the y-axis, ordered so that the word with the largest number of occurrences is plotted first, the word with second largest number of occurrences is plotted second, and so on.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">figure(figsize=(<span class="number">5.5</span>,<span class="number">5</span>))</div><div class="line">r=arange(<span class="number">1</span>,len(nw)+<span class="number">1</span>)</div><div class="line">plot(r,nw,<span class="string">'o'</span>);</div></pre></td></tr></table></figure>
<p><img src="output_10_0.png" alt="png"></p>
<p>As we notice from the graph above it is difficult to see the overall structure of this data since most of the values are so small. A standard method is to plot using logarithmic scales, so that a fixed distance along the axis corresponds to say a factor of 10 (rather than just adding 10). This can be implemented by adding xscale(‘log’) or yscale(‘log’), or both, before or after the plot() command; or by using loglog() instead of plot(). They give the same type of plot.</p>
<p>Now, we extract the counts for the top 500 words in our thewords dictionary, and plot those counts against the word ranks. We also annotate the top ten words on the graph with what they are. Finally we try to fit the data we have plotted with a straight line.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">figure(figsize=(<span class="number">6</span>,<span class="number">6</span>))</div><div class="line">r=<span class="number">1</span>+arange(<span class="number">500</span>)</div><div class="line">loglog(r,nw[:<span class="number">500</span>],<span class="string">'o'</span>)</div><div class="line">plot(r,<span class="number">6500.</span>/r)</div><div class="line"><span class="keyword">for</span> x,y,w <span class="keyword">in</span> list(zip(r,nw,words))[:<span class="number">11</span>]:</div><div class="line"><span class="comment">#    text(x,(1.1 if x%2 else .75)*y,w,ha='center')</span></div><div class="line">    text(x,(<span class="number">1.5</span> <span class="keyword">if</span> x%<span class="number">2</span> <span class="keyword">else</span> <span class="number">.67</span>)*y,w,ha=<span class="string">'center'</span>)</div><div class="line">xlim(<span class="number">.9</span>,<span class="number">1e3</span>)</div><div class="line">ylim(<span class="number">.9</span>,<span class="number">1e4</span>)</div><div class="line">xlabel(<span class="string">'word rank'</span>)</div><div class="line">ylabel(<span class="string">'#occurrences'</span>)</div><div class="line">legend([<span class="string">'data'</span>,<span class="string">'-1 power law "fit"'</span>]);</div></pre></td></tr></table></figure>
<p><img src="output_13_0.png" alt="png"></p>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>Many data points do fall roughly along a straight line in log-log space (Zipf’s law), even for this tiny amount of data, which amount only to the sixty short training set articles.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The linguist George Kingsley Zipf, who theorised that given a large body of language (that is, a long book — or every word uttered by Plus employees during the day), the frequency of each word is close to inversely proportional to its rank in the frequency table.&lt;/p&gt;
&lt;p&gt;In this guide we’ll write some code to look at some word distributions and power laws.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://.alexsinfarosa.com/tags/Machine-Learning/"/>
    
      <category term="Python" scheme="http://.alexsinfarosa.com/tags/Python/"/>
    
      <category term="Zipf&#39;s Law" scheme="http://.alexsinfarosa.com/tags/Zipf-s-Law/"/>
    
      <category term="Power Law" scheme="http://.alexsinfarosa.com/tags/Power-Law/"/>
    
      <category term="Log-Log Plot" scheme="http://.alexsinfarosa.com/tags/Log-Log-Plot/"/>
    
  </entry>
  
  <entry>
    <title>Naive Bayes Text Classifier</title>
    <link href="http://.alexsinfarosa.com/2016/02/21/Naive-Bayes-text-classifier/"/>
    <id>http://.alexsinfarosa.com/2016/02/21/Naive-Bayes-text-classifier/</id>
    <published>2016-02-21T21:49:18.000Z</published>
    <updated>2016-09-30T15:11:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>The following code will attempt to illustrate how to apply the “Naive Bayes” methodology to construct a simple text classifier.</p>
<a id="more"></a>
<p>The <a href="https://github.com/alexsinfarosa/Baby-Machine-Learning/tree/master/data" target="_blank" rel="external">data</a> folder contains 71 files:</p>
<ul>
<li>thirty files labelled p0.txt – p29.txt, with text extracted from ten recent news articles about politics</li>
<li>thirty files labelled s0.txt – s29.txt, with text extracted from ten recent news articles about sports</li>
<li>ten files labelled t0.txt – t9.txt, which will be the “test set”</li>
</ul>
<p>The data were pulled down on 10 Feb 2016, so dominated by Iowa and New Hampshire primaries on the politics side, and by the SuperBowl on the sports side.</p>
<p>We will train a binary classifier (politics/sports) based on the first sixty “training” files (p0.txt – p29.txt and s0.txt – s29.txt), and test it on the ten “test” files (t0.txt – t9.txt).</p>
<h2 id="Import-the-texts-into-our-notebook"><a href="#Import-the-texts-into-our-notebook" class="headerlink" title="Import the texts into our notebook."></a>Import the texts into our notebook.</h2><p>Their contents can be imported into three python lists as follows:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ptexts=[open(<span class="string">'data/p&#123;&#125;.txt'</span>.format(i)).read() <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">30</span>)]</div><div class="line">stexts=[open(<span class="string">'data/s&#123;&#125;.txt'</span>.format(i)).read() <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">30</span>)]</div><div class="line">ttexts=[open(<span class="string">'data/t&#123;&#125;.txt'</span>.format(i)).read() <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)]</div></pre></td></tr></table></figure>
<p>So, the file p0.txt is now available to us as ptexts[0], in fact we could try to view its content by typing:  <code>print(ptexts[0])</code></p>
<p>At this point we need to split the texts into something like words. For this purpose, characters are all made lowercase, then all strings of letters a-z plus apostrophes can be extracted as a list. For example, for the first political text ptexts[0], and using a regular expresion that finds all strings with a-z and ‘:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">re.findall(<span class="string">"[a-z']+"</span>,ptexts[<span class="number">0</span>].lower())</div></pre></td></tr></table></figure>
<p><code>re.findall(&quot;[a-z&#39;]+&quot;,txt)</code> just finds all contiguous strings made up of the characters a-z or ‘, and returns them as a list.</p>
<p>This will return a list of all the ‘words’ in that document. For the time being, we only want to count the number of documents in which a word occurs (not the number of occurrences within the document), so we apply set() to the above list so that each word only appears once. Finally, it’s convenient to use the Counter object (<a href="http://docs.python.org/2/library/collections.html" target="_blank" rel="external">http://docs.python.org/2/library/collections.html</a>), which is just a dictionary to accumulate counts for the words. The result is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</div><div class="line">Counter(set(re.findall(<span class="string">"[a-z']+"</span>,ptexts[<span class="number">0</span>].lower())))</div></pre></td></tr></table></figure>
<h2 id="Define-two-counters"><a href="#Define-two-counters" class="headerlink" title="Define two counters"></a>Define two counters</h2><p>In order to accumulate the number counts (number of documents in which word occurs), we’ll define two counters, n_p for the number counts of words in the politics training set, and n_s for the number counts in the sports training set, by iterating through the associated training sets:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">n_p = np.sum([Counter(set(re.findall(<span class="string">"[a-z']+"</span>,txt.lower()))) <span class="keyword">for</span> txt <span class="keyword">in</span> ptexts])</div><div class="line">n_s = np.sum([Counter(set(re.findall(<span class="string">"[a-z']+"</span>,txt.lower()))) <span class="keyword">for</span> txt <span class="keyword">in</span> stexts])</div></pre></td></tr></table></figure>
<font size="-1">[Note: the above requires having numpy functions loaded in the primary namespace, e.g., using <code>%pylab inline</code> (executes <code>from numpy import *</code>, populating the namespace with numpy functions). We could also use instead, as shown above, <code>import numpy as np</code>, but then we need <code>np.sum()</code> in the above since the standard <code>sum()</code> does not support adding <code>Counter()</code> objects. Something to remember is that each <code>Counter()</code> is a dictionary whose keys are words, and associated values are their number counts, so the <code>sum()</code> has to be smart enough to know that adding <code>Counter()</code>s means taking the set union of their keys and adding the count values of any coincident ones. The generic python <code>sum()</code> does not do this (instead generates a “TypeError: unsupported operand type(s) for +”), but the numpy <code>sum()</code> does work properly for this.]</font>

<p>The Counter object has a convenient ‘most_common’ method, so we can look at the numbers for the most frequent words (where 30 means they occurred in all thirty training documents):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">n_p.most_common(<span class="number">10</span>),n_s.most_common(<span class="number">10</span>)</div></pre></td></tr></table></figure>
<h2 id="Write-the-classifier"><a href="#Write-the-classifier" class="headerlink" title="Write the classifier"></a>Write the classifier</h2><p>The only thing left at this point is to write the classifier code to implement the Naive Bayes classifier. The formula is the following:</p>
<p><small><br>$$<br>p({\rm politics}\ |\ words) = \frac{p(words\ |\ {\rm politics})\,p({\rm politics})}<br>{p(words\ |\ {\rm politics})\,p({\rm politics})+p(words\ |\ {\rm sports})\,p({\rm sports})}<br>\approx\frac{\prod_i p(w_i\ |\ {\rm politics})}{\prod_i p(w_i\ |\ {\rm politics})+\prod_i p(w_i\ |\ {\rm sports})}<br>$$<br></small></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">p_train=<span class="number">30.</span></div><div class="line">s_train=<span class="number">30.</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bayes_classifier</span><span class="params">(txt)</span>:</span></div><div class="line">    p=<span class="number">1.</span></div><div class="line">    s=<span class="number">1.</span></div><div class="line">    word_counts=Counter(re.findall(<span class="string">"[a-z']+"</span>,txt.lower()))</div><div class="line">    <span class="comment"># We need to account for stopwords</span></div><div class="line">    keywords=[w <span class="keyword">for</span> w,c <span class="keyword">in</span> word_counts.most_common() <span class="keyword">if</span> n_p[w]&lt; <span class="number">26</span> <span class="keyword">and</span> n_s[w]&lt;<span class="number">26</span>][:<span class="number">30</span>]</div><div class="line">    print(<span class="string">'\tbased on "&#123;&#125; ...":'</span>.format(<span class="string">', '</span>.join(keywords[:<span class="number">9</span>])))</div><div class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> keywords:</div><div class="line">        <span class="comment"># handling words in the test documents that haven't been seen before (".1 smoothing")</span></div><div class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> n_p: n_p[word]=<span class="number">.1</span></div><div class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> n_s: n_s[word]=<span class="number">.1</span></div><div class="line">        p *= n_p[word]/p_train</div><div class="line">        s *= n_s[word]/s_train</div><div class="line">    prob=p/(p+s)</div><div class="line">    <span class="keyword">if</span> prob &gt;= <span class="number">.5</span>:</div><div class="line">        <span class="keyword">return</span> <span class="string">'POLITICS'</span>,prob</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> <span class="string">'SPORT'</span>,<span class="number">1</span>-prob</div></pre></td></tr></table></figure>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">    print(<span class="string">'t&#123;&#125;.txt:'</span>.format(i),bayes_classifier(ttexts[i]))</div></pre></td></tr></table></figure>
<pre><code>    based on &quot;rubio, him, rubio&apos;s, he&apos;s, there&apos;s, party, question, suggest, braman ...&quot;:
t0.txt: (&apos;POLITICS&apos;, 0.9999999999843888)
    based on &quot;i, warriors, ever, spurs, about, i&apos;d, greatest, were, been ...&quot;:
t1.txt: (&apos;SPORT&apos;, 1.0)
    based on &quot;saban, alabama, football, coach, college, team, national, state, season ...&quot;:
t2.txt: (&apos;SPORT&apos;, 1.0)
    based on &quot;trump, iowa, will, cruz, gop, republican, trump&apos;s, rubio, there ...&quot;:
t3.txt: (&apos;POLITICS&apos;, 1.0)
    based on &quot;sanders, democratic, left, party, win, will, s, people, i ...&quot;:
t4.txt: (&apos;POLITICS&apos;, 1.0)
    based on &quot;georgia, world, russia, georgia&apos;s, rugby, team, cup, war, sharikadze ...&quot;:
t5.txt: (&apos;SPORT&apos;, 0.9999998083467077)
    based on &quot;bush, campaign, hampshire, about, jeb, trump, him, he&apos;s, its ...&quot;:
t6.txt: (&apos;POLITICS&apos;, 0.9999999999999415)
    based on &quot;curry, play, season, after, been, steph, five, team, warriors ...&quot;:
t7.txt: (&apos;SPORT&apos;, 0.9999999999999866)
    based on &quot;scholarships, college, athletes, players, year, scholarship, her, could, athletic ...&quot;:
t8.txt: (&apos;SPORT&apos;, 0.999999999704003)
    based on &quot;golf, woods, tiger, no, championship, best, two, player, age ...&quot;:
t9.txt: (&apos;SPORT&apos;, 1.0)
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The following code will attempt to illustrate how to apply the “Naive Bayes” methodology to construct a simple text classifier.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://.alexsinfarosa.com/tags/Python/"/>
    
      <category term="Naive Bayes" scheme="http://.alexsinfarosa.com/tags/Naive-Bayes/"/>
    
      <category term="Text Classifier" scheme="http://.alexsinfarosa.com/tags/Text-Classifier/"/>
    
  </entry>
  
</feed>
