<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="author" content="Alex Sinfarosa"><meta name="description" content="Thefollowingcodewillattempttoillustratehowtoapplythe“NaiveBayes”methodologytoconstructasimpletextclassifier."><meta name="keywords" content="Python,Naive Bayes,Text Classifier"><title>Naive Bayes Text Classifier · Alex Sinfarosa</title><link rel="icon" href="/"><link rel="canonical" href="http://.alexsinfarosa.com/2016/02/21/Naive-Bayes-text-classifier/"><link rel="alternate" href="/atom.xml" title="Alex Sinfarosa"><link rel="stylesheet" href="/fonts/iconfont/iconfont.css"><link rel="stylesheet" href="/css/style.css"><script type="text/javascript"><!-- hexo-inject:begin --><!-- hexo-inject:end -->var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?Your baidu Analytics ID";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script type="text/javascript">(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-84813198-1', 'auto');
ga('send', 'pageview');</script><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><div id="main"><header><a href="/." class="logo">Alex Sinfarosa</a><ul class="nav"><li class="nav-link"><a href="/" target="_self">Home</a></li><li class="nav-link"><a href="/archives/" target="_self">Archives</a></li><li class="nav-link"><a href="/tags/" target="_self">Tags</a></li><li class="nav-link"><a href="/about/" target="_self">About</a></li></ul></header><section id="container"><article class="post"><h1 class="post-title">Naive Bayes Text Classifier</h1><span class="post-time">Feb 21, 2016</span><div id="sidebar" class="post-sidebar"><h3 class="heading">Contents</h3><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Import-the-texts-into-our-notebook"><span class="toc-text">Import the texts into our notebook.</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Define-two-counters"><span class="toc-text">Define two counters</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Write-the-classifier"><span class="toc-text">Write the classifier</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Results"><span class="toc-text">Results</span></a></li></ol></div><div class="post-content"><p>The following code will attempt to illustrate how to apply the “Naive Bayes” methodology to construct a simple text classifier.</p>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><a id="more"></a>
<p>The <a href="https://github.com/alexsinfarosa/Baby-Machine-Learning/tree/master/data" target="_blank" rel="external">data</a> folder contains 71 files:</p>
<ul>
<li>thirty files labelled p0.txt – p29.txt, with text extracted from ten recent news articles about politics</li>
<li>thirty files labelled s0.txt – s29.txt, with text extracted from ten recent news articles about sports</li>
<li>ten files labelled t0.txt – t9.txt, which will be the “test set”</li>
</ul>
<p>The data were pulled down on 10 Feb 2016, so dominated by Iowa and New Hampshire primaries on the politics side, and by the Super Bowl on the sports side.</p>
<p>We will train a binary classifier (politics/sports) based on the first sixty “training” files (p0.txt – p29.txt and s0.txt – s29.txt), and test it on the ten “test” files (t0.txt – t9.txt).</p>
<h2 id="Import-the-texts-into-our-notebook"><a href="#Import-the-texts-into-our-notebook" class="headerlink" title="Import the texts into our notebook."></a>Import the texts into our notebook.</h2><p>Their contents can be imported into three python lists as follows:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ptexts=[open(<span class="string">'data/p&#123;&#125;.txt'</span>.format(i)).read() <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">30</span>)]</div><div class="line">stexts=[open(<span class="string">'data/s&#123;&#125;.txt'</span>.format(i)).read() <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">30</span>)]</div><div class="line">ttexts=[open(<span class="string">'data/t&#123;&#125;.txt'</span>.format(i)).read() <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)]</div></pre></td></tr></table></figure>
<p>So, the file p0.txt is now available to us as ptexts[0], in fact we could try to view its content by typing:  <code>print(ptexts[0])</code></p>
<p>At this point we need to split the texts into something like words. For this purpose, characters are all made lowercase, then all strings of letters a-z plus apostrophes can be extracted as a list. For example, for the first political text ptexts[0], and using a regular expresion that finds all strings with a-z and ‘:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">re.findall(<span class="string">"[a-z']+"</span>,ptexts[<span class="number">0</span>].lower())</div></pre></td></tr></table></figure>
<p><code>re.findall(&quot;[a-z&#39;]+&quot;,txt)</code> just finds all contiguous strings made up of the characters a-z or ‘, and returns them as a list.</p>
<p>This will return a list of all the ‘words’ in that document. For the time being, we only want to count the number of documents in which a word occurs (not the number of occurrences within the document), so we apply set() to the above list so that each word only appears once. Finally, it’s convenient to use the Counter object (<a href="http://docs.python.org/2/library/collections.html" target="_blank" rel="external">http://docs.python.org/2/library/collections.html</a>), which is just a dictionary to accumulate counts for the words. The result is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</div><div class="line">Counter(set(re.findall(<span class="string">"[a-z']+"</span>,ptexts[<span class="number">0</span>].lower())))</div></pre></td></tr></table></figure>
<h2 id="Define-two-counters"><a href="#Define-two-counters" class="headerlink" title="Define two counters"></a>Define two counters</h2><p>In order to accumulate the number counts (number of documents in which word occurs), we’ll define two counters, n_p for the number counts of words in the politics training set, and n_s for the number counts in the sports training set, by iterating through the associated training sets:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">n_p = np.sum([Counter(set(re.findall(<span class="string">"[a-z']+"</span>,txt.lower()))) <span class="keyword">for</span> txt <span class="keyword">in</span> ptexts])</div><div class="line">n_s = np.sum([Counter(set(re.findall(<span class="string">"[a-z']+"</span>,txt.lower()))) <span class="keyword">for</span> txt <span class="keyword">in</span> stexts])</div></pre></td></tr></table></figure>
<font size="-1">[Note: the above requires having numpy functions loaded in the primary namespace, e.g., using <code>%pylab inline</code> (executes <code>from numpy import *</code>, populating the namespace with numpy functions). We could also use instead, as shown above, <code>import numpy as np</code>, but then we need <code>np.sum()</code> in the above since the standard <code>sum()</code> does not support adding <code>Counter()</code> objects. Something to remember is that each <code>Counter()</code> is a dictionary whose keys are words, and associated values are their number counts, so the <code>sum()</code> has to be smart enough to know that adding <code>Counter()</code>s means taking the set union of their keys and adding the count values of any coincident ones. The generic python <code>sum()</code> does not do this (instead generates a “TypeError: unsupported operand type(s) for +”), but the numpy <code>sum()</code> does work properly for this.]</font>

<p>The Counter object has a convenient ‘most_common’ method, so we can look at the numbers for the most frequent words (where 30 means they occurred in all thirty training documents):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">n_p.most_common(<span class="number">10</span>),n_s.most_common(<span class="number">10</span>)</div></pre></td></tr></table></figure>
<h2 id="Write-the-classifier"><a href="#Write-the-classifier" class="headerlink" title="Write the classifier"></a>Write the classifier</h2><p>The only thing left at this point is to write the classifier code to implement the Naive Bayes classifier. The formula is the following:</p>
<p><small><br>$$<br>p({\rm politics}\ |\ words) = \frac{p(words\ |\ {\rm politics})\,p({\rm politics})}<br>{p(words\ |\ {\rm politics})\,p({\rm politics})+p(words\ |\ {\rm sports})\,p({\rm sports})}<br>\approx\frac{\prod_i p(w_i\ |\ {\rm politics})}{\prod_i p(w_i\ |\ {\rm politics})+\prod_i p(w_i\ |\ {\rm sports})}<br>$$<br></small></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">p_train=<span class="number">30.</span></div><div class="line">s_train=<span class="number">30.</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bayes_classifier</span><span class="params">(txt)</span>:</span></div><div class="line">    p=<span class="number">1.</span></div><div class="line">    s=<span class="number">1.</span></div><div class="line">    word_counts=Counter(re.findall(<span class="string">"[a-z']+"</span>,txt.lower()))</div><div class="line">    <span class="comment"># We need to account for stopwords</span></div><div class="line">    keywords=[w <span class="keyword">for</span> w,c <span class="keyword">in</span> word_counts.most_common() <span class="keyword">if</span> n_p[w]&lt; <span class="number">26</span> <span class="keyword">and</span> n_s[w]&lt;<span class="number">26</span>][:<span class="number">30</span>]</div><div class="line">    print(<span class="string">'\tbased on "&#123;&#125; ...":'</span>.format(<span class="string">', '</span>.join(keywords[:<span class="number">9</span>])))</div><div class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> keywords:</div><div class="line">        <span class="comment"># handling words in the test documents that haven't been seen before (".1 smoothing")</span></div><div class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> n_p: n_p[word]=<span class="number">.1</span></div><div class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> n_s: n_s[word]=<span class="number">.1</span></div><div class="line">        p *= n_p[word]/p_train</div><div class="line">        s *= n_s[word]/s_train</div><div class="line">    prob=p/(p+s)</div><div class="line">    <span class="keyword">if</span> prob &gt;= <span class="number">.5</span>:</div><div class="line">        <span class="keyword">return</span> <span class="string">'POLITICS'</span>,prob</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> <span class="string">'SPORT'</span>,<span class="number">1</span>-prob</div></pre></td></tr></table></figure>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">    print(<span class="string">'t&#123;&#125;.txt:'</span>.format(i),bayes_classifier(ttexts[i]))</div></pre></td></tr></table></figure>
<pre><code>    based on &quot;rubio, him, rubio&apos;s, he&apos;s, there&apos;s, party, question, suggest, braman ...&quot;:
t0.txt: (&apos;POLITICS&apos;, 0.9999999999843888)
    based on &quot;i, warriors, ever, spurs, about, i&apos;d, greatest, were, been ...&quot;:
t1.txt: (&apos;SPORT&apos;, 1.0)
    based on &quot;saban, alabama, football, coach, college, team, national, state, season ...&quot;:
t2.txt: (&apos;SPORT&apos;, 1.0)
    based on &quot;trump, iowa, will, cruz, gop, republican, trump&apos;s, rubio, there ...&quot;:
t3.txt: (&apos;POLITICS&apos;, 1.0)
    based on &quot;sanders, democratic, left, party, win, will, s, people, i ...&quot;:
t4.txt: (&apos;POLITICS&apos;, 1.0)
    based on &quot;georgia, world, russia, georgia&apos;s, rugby, team, cup, war, sharikadze ...&quot;:
t5.txt: (&apos;SPORT&apos;, 0.9999998083467077)
    based on &quot;bush, campaign, hampshire, about, jeb, trump, him, he&apos;s, its ...&quot;:
t6.txt: (&apos;POLITICS&apos;, 0.9999999999999415)
    based on &quot;curry, play, season, after, been, steph, five, team, warriors ...&quot;:
t7.txt: (&apos;SPORT&apos;, 0.9999999999999866)
    based on &quot;scholarships, college, athletes, players, year, scholarship, her, could, athletic ...&quot;:
t8.txt: (&apos;SPORT&apos;, 0.999999999704003)
    based on &quot;golf, woods, tiger, no, championship, best, two, player, age ...&quot;:
t9.txt: (&apos;SPORT&apos;, 1.0)
</code></pre></div></article><div class="tags"><a href="/tags/Python/">Python</a><a href="/tags/Naive-Bayes/">Naive Bayes</a><a href="/tags/Text-Classifier/">Text Classifier</a></div><div class="paginator"><a href="/2016/03/16/Zipf-s-Law/" class="prev"><i class="iconfont icon-left"></i><span> Prev</span></a></div><section id="comments"><div id="disqus_thread"></div></section><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'http://.alexsinfarosa.com/2016/02/21/Naive-Bayes-text-classifier/';
    this.page.identifier = '2016/02/21/Naive-Bayes-text-classifier/';
    this.page.title = 'Naive Bayes Text Classifier';
};
(function() {
var d = document, s = d.createElement('script');

s.src = '//alexsinfarosa-com.disqus.com/embed.js';

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();</script></section><footer><div class="copyright"><p class="power">Powered by <a href="https://hexo.io/">Hexo</a> and Theme by <a href="https://github.com/ahonn/hexo-theme-even"> Even</a></p><p class="since">&copy;2016<span class="heart"><i class="iconfont icon-heart"></i></span><span class="author">Alex Sinfarosa</span></p></div><label id="back2top"><i class="iconfont icon-up"></i></label></footer></div><script src="/js/zepto.min.js"></script><script src="/js/theme.js"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body></html>